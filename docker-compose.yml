# -------------------- VOLUMES & NETWORKS --------------------
volumes:
  spark-data:        # Named volume for Spark to persist logs/data
  kafka_data:        # Named volume for Kafka log and topic data
  ivy_cache:         # Named volume for pyspark installed packages
  postgres_data:
  grafana:

networks:
  stock_data:       # Shared network for containers communication
  monitoring:

services:
  spark-master:
    image: spark:3.5.1-python3               # Use the official Apache Spark image with Python 3 support (v3.5.7)
    container_name: spark-master             # Assign a fixed, human-readable name to this container
    command: [                           # Override the default container command to start Spark master manually
      "/opt/spark/bin/spark-class", 
      "org.apache.spark.deploy.master.Master",  
      "--host", "spark-master",                    
      "--port", "7077",                            
      "--webui-port", "8080"]    # Set the hostname that other nodes use to reach the master
    environment:
      - SPARK_MODE=master                    # (Optional variable) indicates this container acts as master
      - SPARK_MASTER_HOST=spark-master       # Hostname of the Spark master, used by workers to locate it
    ports:
      - "8081:8080"                          # Expose master web UI (container 8080) on host port 8081
      - "7077:7077"                          # Expose Spark master service port for worker/job connections
    volumes:
      - spark-data:/opt/spark-data           # Mount a named volume to persist Spark logs or data
    networks:
      - stock_data                          # Connect the container to a shared Docker network for inter-service communication

  # ------------------------------------------------------------
  # Spark Worker Node
  # ------------------------------------------------------------
  spark-worker:
    image: spark:3.5.1-python3             
    container_name: spark-worker             # Name for the worker container
    command: [                           # Override the default container command to start Spark master manually
      "/opt/spark/bin/spark-class", 
      "org.apache.spark.deploy.worker.Worker",  
      "spark://spark-master:7077",                           
      "--webui-port", "8081"] 
    environment:
      - SPARK_WORKER_CORES=2                 # Allocate 2 CPU cores to this worker
      - SPARK_WORKER_MEMORY=2G               # Allocate 2 GB of memory to this worker
    depends_on:
      - spark-master                         # Ensure Spark master starts before the worker tries to connect
    volumes:
      - spark-data:/opt/spark-data           # Share the same named volume for logs/data if needed
    networks:
      - stock_data                          # Attach to same network so it can communicate with the master

  consumer:
    build:
      context: ./consumer              # Use same Apache Spark image for PySpark streaming
      dockerfile: Dockerfile
    image: stream_consumer
    container_name: consumer          # Assign a name for the streaming job container
    depends_on:
      - spark-master
      - kafka                  # Wait for Kafka and Spark to be online before starting job
    env_file: .env
    restart: unless-stopped
    volumes:
      - ivy_cache:/opt/spark/work-dir/.ivy2
      - ./consumer:/opt/spark/work-dir/apps
    networks:
      - stock_data  

    # -------------------- KAFKA SERVICE --------------------
  kafka:
    build: ./kafka   
    image: kafka_monitor:v1               # Use the Confluent Kafka image (version 7.4.10)
    container_name: kafka                 # Explicit container name for easier reference
    hostname: kafka                       # Hostname used inside Docker network
    ports:
      - "9092:9092"                       # Internal listener for Docker containers
      - "9094:9094"                       # External listener for local host (Python clients)
      - "7071:7071"                       # JMX exporter metrics port for Prometheus
    environment:
      KAFKA_KRAFT_MODE: "true"            # Enable KRaft mode (no Zookeeper)
      KAFKA_PROCESS_ROLES: controller,broker   # Kafka acts as both controller and broker
      KAFKA_NODE_ID: 1                    # Unique ID for this Kafka node
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093" # Define controller voters (only one here)
            
      KAFKA_LISTENERS: PLAINTEXT_INTERNAL://0.0.0.0:9092,PLAINTEXT_EXTERNAL://0.0.0.0:9094,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT_INTERNAL://kafka:9092,PLAINTEXT_EXTERNAL://localhost:9094

      # Map listener names to security protocols
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT_INTERNAL:PLAINTEXT,PLAINTEXT_EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL   # Brokers communicate via internal listener
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER            # Controller listener name
      KAFKA_LOG_DIRS: /var/lib/kafka/data       # Directory for Kafka logs (message data)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"   # Allow automatic topic creation
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # Replication factor = 1 since single broker
      KAFKA_LOG_RETENTION_HOURS: 168            # Keep messages for 7 days
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # No delay before consumer group rebalance
      CLUSTER_ID: "x5r9Du-wS4KSPwPBJ-Fh1Q"      # Unique cluster ID for KRaft mode 
      # generate cluster_id  docker: run --rm confluentinc/cp-kafka:7.4.10 kafka-storage random-uuid
      KAFKA_OPTS: -javaagent:/usr/app/jmx_prometheus_javaagent.jar=7071:/usr/app/jmx-agent-config.yml
  
    volumes:
      - kafka_data:/var/lib/kafka/data          # Persist Kafka message data
    
    networks:
      - stock_data                             # Attach Kafka to shared network for UI and Spark
      - monitoring

  # -------------------- KAFKA UI SERVICE --------------------
  kafka-ui:
    container_name: kafka-ui                    # Name for the Kafka UI container
    image: provectuslabs/kafka-ui:v0.7.2        # Use Provectus Kafka UI (open-source web UI)
    ports:
      - 8085:8080                               # Map Kafka UI web interface to localhost:8085
    depends_on:
      - kafka                                   # Ensure Kafka starts before the UI
    environment:
      KAFKA_CLUSTERS_0_NAME: stock_data              # Name to display in the UI
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092  # Internal Kafka connection (inside Docker)
      DYNAMIC_CONFIG_ENABLED: 'true'            # Allow live config changes in the UI
    networks:
      - stock_data                             # Attach UI to same network as Kafka


  postgres:
    image: debezium/postgres:17
    container_name: postgres_db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: stock_data
    ports:
      - "5434:5432"
    volumes:
      - postgres_data:/var/lib/postgres/data
    networks:
      - stock_data
      - monitoring

  postgres_exporter:
    image: prometheuscommunity/postgres-exporter:v0.17.1
    container_name: postgres_exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/stock_data?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      - postgres
    networks:
      - monitoring

  kafka-exporter:
    image: danielqsj/kafka-exporter:v1.9.0
    restart: unless-stopped
    depends_on:
      - kafka
    ports:
      - "9308:9308"
    command: ["--kafka.server=kafka:9092"]
    networks:
      - monitoring

  pgadmin:
    image: dpage/pgadmin4:9
    container_name: pgadmin
    restart: unless-stopped
    environment: 
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - stock_data

  prometheus:
    image: prom/prometheus:v3.5.0
    container_name: prometheus
    hostname: prometheus
    restart: unless-stopped
    tty: true
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:12.0
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_AUTH_PROXY_ENABLED=true
      - GF_PATHS_PROVISIONING=/var/lib/grafana/provisioning/
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
    volumes:
      - grafana:/var/lib/grafana/
      - ./grafana:/etc/grafana/provisioning/datasources
    networks:
      - monitoring
